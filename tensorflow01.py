# -*- coding: utf-8 -*-
"""TensorFlow01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ce5cZhzrv3XsfdMT66ulSF7_F_RdEpB8

# Module 1 : Installing the Tensorflow

**<h1>How to import tensorflow</h1>**
"""

import tensorflow as tf 
print(tf.version)

"""**<h1>Creating Tensors</h1>**
Below is an example of how to create some different tensors.

You simply define the value of the tensor and the datatype and you are good to go! It's worth mentioning that usually we deal with tensors of numeric data, it is quite rare to see string tensors.
"""

string = tf.Variable("this is a string", tf.string) 
number = tf.Variable(324, tf.int16)
floating = tf.Variable(3.567, tf.float64)

"""**<h1>Rank of tensor</h1>**<br>
It is the no. of dimensions involved in the tensor.<br>
The rank of a tensor is direclty related to the deepest level of nested lists. <br>You can see in the first example ["Test"] is a rank 1 tensor as the deepest level of nesting is 1. Where in the second example [["test", "ok"], ["test", "yes"]] is a rank 2 tensor as the deepest level of nesting is 2.
"""

rank1_tensor = tf.Variable(["Test"], tf.string) 
rank2_tensor = tf.Variable([["test","OK", "ok"], ["test","OK", "yes"]], tf.string)
tf.rank(rank2_tensor)
rank2_tensor.shape  #Rerun the code with 'rank2_tensor'

"""**<h1>  Shape of tensor</h1>**
The shape of a tensor is simply the number of elements that exist in each dimension. TensorFlow will try to determine the shape of a tensor but sometimes it may be unknown.<br>
**x = no of lists**<br>
**y=no of the nested list(cells) in the main list (x)**<br>
**z=no. of digits or variables in the cell**<br> 
"""

tensor1 = tf.ones([1,2,3])  # tf.ones() creates a shape [1,2,3] tensor full of ones (try [2,2,1] and [1,2,1])
print(tensor1)               # tf.ones([x,y,z])

"""Reshape

reshape existing data to shape [2,3,1]<br>
-1 tells the tensor to calculate the size of the dimension in that place this <br>will reshape the tensor to [3,2]<br>
The number of elements in the reshaped tensor MUST match the number in the original
"""

tensor1 = tf.ones([1,2,3])
tensor2 = tf.reshape(tensor1, [2,3,1])
tensor3 = tf.reshape(tensor2, [3, -1])       #-1 tells the tensor to calculate the size of the dimension in that place this will reshape the tensor to [3,2]                    
print(tensor3)

"""Slicing Tensors<br>
You may be familiar with the term "slice" in python and its use on lists, tuples etc. Well the slice operator can be used on tensors to select specific axes or elements.

When we slice or select elements from a tensor, we can use comma seperated values inside the set of square brackets. Each subsequent value refrences a different dimension of the tensor.

Ex: tensor[dim1, dim2, dim3]

I've included a few examples that will hopefully help illustrate how we can manipulate tensors with the slice operator.
"""

# Creating a 2D tensor
matrix = [[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15],[16,17,18,19,20]]

tensor = tf.Variable(matrix, dtype=tf.int32) 
print(tf.rank(tensor))
print(tensor.shape)

# Now lets select some different rows and columns from our tensor

three = tensor[0,2]  # selects the 3rd element from the 1st row
print(three)  # -> 3

row1 = tensor[0]  # selects the first row
print(row1)

column1 = tensor[:, 0]  # selects the first column
print(column1)

row_2_and_4 = tensor[1::2]  # selects second and fourth row
print(2,4)

column_1_in_row_2_and_3 = tensor[1:3, 0]
print(column_1_in_row_2_and_3)

"""Types of Tensors<br>
Before we go to far, I will mention that there are diffent types of tensors. These are the most used and we will talk more in depth about each as they are used.
Variable<br>
Constant<br>
Placeholder<br>
SparseTensor<br>
With the execption of Variable all these tensors are immuttable, meaning their value may not change during execution.

For now, it is enough to understand that we use the Variable tensor when we want to potentially change the value of our tensor.
"""

t = tf.zeros([5,5,5,5])#this will have 5x5x5x5 elements in it which is 625
print(t)
t =tf.reshape(t,[625])  
print(t)
t =tf.reshape(t,[125,-1])  
print(t)



"""# Module 2 : Algorithm

<h1>TensorFlow Core Learning Algorithms</h1>

---


In this notebook we will walk through 4 fundemental machine learning algorithms. We will apply each of these algorithms to unique problems and datasets before highlighting the use cases of each.

The algorithms we will focus on include:

Linear Regression<br>
Classification<br>
Clustering<br>
Hidden Markov Models<br>
It is worth noting that there are many tools within TensorFlow that could be used to solve the problems we will see below. I have chosen the tools that I belive give the most variety and are easiest to use.

<h2>Linear Regression</h2>

---
Linear regression is one of the most basic forms of machine learning and is used to predict numeric values.
In this tutorial we will use a linear model to predict the survival rate of passangers from the titanic dataset.
This section is based on the following documentation: https://www.tensorflow.org/tutorials/estimator/linear

<H1>Import </H1>
"""

!pip install -q sklearn

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import clear_output
import tensorflow as tf
from tensorflow import *
from six.moves import urllib
import tensorflow.compat.v2.feature_column as fc

#2 Degree dataset   
import matplotlib.pyplot as plt
import numpy as np
x = [0,1, 5, 3, 4]                     #try with Random values
y = [0,1, 2, 1.5, 4]                   #try with Random values
plt.plot(x, y, 'ro')

"""We can see that this data has a linear coorespondence. When the x value increases, so does the y. Because of this relation we can create a line of best fit for this dataset. In this example our line will only use one input variable, as we are working with two dimensions. In larger datasets with more features our line will have more features and inputs.

Here's a refresher on the equation of a line in 2D.

$ y = mx + b $

Here's an example of a line of best fit for this graph.

"""

plt.plot(x, y, 'ro')
plt.axis([0, 6, 0, 6])
plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x))) #to get the line of best fit.
plt.show()

"""**Case 1 :**
Titanic survival case
"""

#Load Dataset (Download this csv)
dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') #Training data
dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data                                                    
print(dftrain.head())
y_train = dftrain.pop('survived')                                                   #This moved the Surviover dataset to y_train 
y_eval = dfeval.pop('survived')                                               
print(dftrain["sex"])                                                               #loc : this gives you specific row of the dataset , "age" : gives you specific columns of dataset
print(y_train)                                                                      #this gives you survivors which we poped in the last head command.
print(dftrain.loc[15],y_train.loc[15])                                                #This gives all the data.

"""The ```pd.read_csv()``` method will return to us a new pandas *dataframe*. You can think of a dataframe like a table. In fact, we can actually have a look at the table representation.

We've decided to pop the "survived" column from our dataset and store it in a new variable. This column simply tells us if the person survived our not.

To look at the data we'll use the ```.head()``` method from pandas. This will show us the first 5 items in our dataframe.

<h1>Segregating the dataset</h1>
"""

dftrain.head()

dftrain.describe()                     #important

dftrain.shape        #627 rows , 9 cloumns

dftrain.age.hist(bins=20)           #age graph

dftrain['class'].value_counts().plot(kind='barh')

dftrain.sex.value_counts().plot(kind='barh')#sex

pd.concat([dftrain,y_train],axis=1).groupby('sex').survived.mean().plot(kind='barh').set_xlabel('%survivor') # %of survivor by sex

dftrain["embark_town"].unique()                                 #to get all the unique values in a cloumn

"""After analyzing this information, we should notice the following:
- Most passengers are in their 20's or 30's 
- Most passengers are male
- Most passengers are in "Third" class
- Females have a much higher chance of survival

### Training vs Testing Data
You may have noticed that we loaded **two different datasets** above. This is because when we train models, we need two sets of data: **training and testing**. 

The **training** data is what we feed to the model so that it can develop and learn. It is usually a much larger size than the testing data.

The **testing** data is what we use to evaulate the model and see how well it is performing. We must use a seperate set of data that the model has not been trained on to evaluate it. Can you think of why this is?

Well, the point of our model is to be able to make predictions on NEW data, data that we have never seen before. If we simply test the model on the data that it has already seen we cannot measure its accuracy accuratly. We can't be sure that the model hasn't simply memorized our training data. This is why we need our testing and training data to be seperate.

###Feature Columns
In our dataset we have two different kinds of information: **Categorical and Numeric**

Our **categorical data** is anything that is not numeric! For example, the sex column does not use numbers, it uses the words "male" and "female".

Before we continue and create/train a model we must convet our categorical data into numeric data. We can do this by encoding each category with an integer (ex. male = 1, female = 2). 

Fortunately for us TensorFlow has some tools to help!
"""

CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',
                       'embark_town', 'alone']
NUMERIC_COLUMNS = ['age', 'fare']

feature_columns = []
for feature_name in CATEGORICAL_COLUMNS:                                        #for loop                          
  vocabulary = dftrain[feature_name].unique()  # gets a list of all unique values from given feature column
  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))

for feature_name in NUMERIC_COLUMNS:
  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))

print(feature_columns)

"""**Creating a Model**"""

linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)

"""**Train the model**"""

linear_est.train(train_input_fn)  # train
result = linear_est.evaluate(eval_input_fn)  # get model metrics/stats by testing on tetsing data

clear_output()  # clears consoke output
print(result['accuracy'])  # the result variable is simply a dict of stats about our model

"""# Module 3 :Testing the final

Let's break this code down a little bit...

Essentially what we are doing here is creating a list of features that are used in our dataset.

The cryptic lines of code inside the append() create an object that our model can use to map string values like "male" and "female" to integers. This allows us to avoid manually having to encode our dataframes.

And here is some relevant documentation

https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list?version=stable

###The Training Process
So, we are almost done preparing our dataset and I feel as though it's a good time to explain how our model is trained. Specifically, how input data is fed to our model. 

For this specific model data is going to be streamed into it in small batches of 32. This means we will not feed the entire dataset to our model at once, but simply small batches of entries. We will feed these batches to our model multiple times according to the number of **epochs**. 

An **epoch** is simply one stream of our entire dataset. The number of epochs we define is the amount of times our model will see the entire dataset. We use multiple epochs in hope that after seeing the same data multiple times the model will better determine how to estimate it.

Ex. if we have 10 ephocs, our model will see the same dataset 10 times. 

Since we need to feed our data in batches and multiple times, we need to create something called an **input function**. The input function simply defines how our dataset will be converted into batches at each epoch.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import clear_output
import tensorflow as tf
from tensorflow import *
from six.moves import urllib
import tensorflow.compat.v2.feature_column as fc

#Load Dataset (Download this csv)
dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') #Training data
dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data                                                    
print(dftrain.head())
y_train = dftrain.pop('survived')                                                   #This moved the Surviover dataset to y_train 
y_eval = dfeval.pop('survived')

CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',
                       'embark_town', 'alone']
NUMERIC_COLUMNS = ['age', 'fare']

feature_columns = []
for feature_name in CATEGORICAL_COLUMNS:                                        #for loop                          
  vocabulary = dftrain[feature_name].unique()  # gets a list of all unique values from given feature column
  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))

for feature_name in NUMERIC_COLUMNS:
  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))

print(feature_columns)

"""###Input Function
The TensorFlow model we are going to use requires that the data we pass it comes in as a ```tf.data.Dataset``` object. This means we must create a *input function* that can convert our current pandas dataframe into that object. 

Below you'll see a seemingly complicated input function, this is straight from the TensorFlow documentation (https://www.tensorflow.org/tutorials/estimator/linear). I've commented as much as I can to make it understandble, but you may want to refer to the documentation for a detailed explination of each method.
"""

#Input data model
def make_input_fn(data_df, label_df, num_epochs=1000, shuffle=True, batch_size=64):
  def input_function():  # inner function, this will be returned
    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  # create tf.data.Dataset object with data and its label
    if shuffle:
      ds = ds.shuffle(1000)  # randomize order of data
    ds = ds.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs
    return ds  # return a batch of the dataset
  return input_function  # return a function object for use

train_input_fn = make_input_fn(dftrain, y_train)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model
eval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)

linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns) 

linear_est.train(train_input_fn)  # train
result = linear_est.evaluate(eval_input_fn)  # get model metrics/stats by testing on tetsing data

clear_output()  # clears consoke output
print(result['accuracy'])  # the result variable is simply a dict of stats about our model
print(result)

result = list(linear_est.predict(eval_input_fn))
print(result[2])
print(dfeval.loc[2])                                 #Prediction of 1st srno. from eval data   
print("Survival chance: ",result[2]['probabilities'][1])              #Chance of survival of srno. one from eval dataset
print("Not Survival chance :",result[2]['probabilities'][0])              #Chance of not survival of srno. one from eval dataset
print("Real ans: ",y_eval.loc[2])

